#!/usr/bin/env python3
"""
Vocabulary Difficulty Assessment Service

This module provides API endpoints to assess vocabulary difficulty
based on multiple criteria including frequency, CEFR levels, and contextual usage.
"""

import os
import json
import requests
from typing import List, Dict, Optional
from flask import Blueprint, request, jsonify
import nltk
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
import re

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

vocab_bp = Blueprint('vocabulary', __name__)

class VocabularyAnalyzer:
    def __init__(self):
        # å°è¯•ä½¿ç”¨ä¸“ä¸šè¯æ±‡æ•°æ®åº“
        try:
            from vocabulary_database import VocabularyDatabase
            self.vocab_db = VocabularyDatabase()
            self.use_database = True
            print("âœ… ä½¿ç”¨ä¸“ä¸šè¯æ±‡æ•°æ®åº“")
        except ImportError:
            self.vocab_db = None
            self.use_database = False
            print("âš ï¸ ä½¿ç”¨å¤‡ç”¨è¯æ±‡åˆ†æ")
            # å¤‡ç”¨æ•°æ®
            self.word_frequency = self._load_word_frequency()
            self.cefr_levels = self._load_cefr_levels()
            self.academic_words = self._load_academic_words()
        
    def _load_word_frequency(self) -> Dict[str, int]:
        """åŠ è½½è¯é¢‘æ•°æ®ï¼ˆåŸºäºGoogle 10000æœ€å¸¸è§è‹±è¯­å•è¯ï¼‰"""
        # è¿™é‡Œå¯ä»¥åŠ è½½çœŸå®çš„è¯é¢‘æ•°æ®æ–‡ä»¶
        # ç®€åŒ–ç¤ºä¾‹ï¼šè¿”å›ä¸€äº›åŸºæœ¬çš„è¯é¢‘æ•°æ®
        common_words = {
            'the': 1, 'and': 2, 'of': 3, 'to': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 
            'on': 9, 'that': 10, 'by': 11, 'this': 12, 'with': 13, 'i': 14, 'you': 15,
            'it': 16, 'not': 17, 'or': 18, 'be': 19, 'are': 20, 'from': 21, 'at': 22,
            'as': 23, 'your': 24, 'all': 25, 'have': 26, 'new': 27, 'more': 28, 'an': 29,
            'was': 30, 'we': 31, 'will': 32, 'home': 33, 'can': 34, 'us': 35, 'about': 36,
            'if': 37, 'page': 38, 'my': 39, 'has': 40, 'search': 41, 'free': 42, 'but': 43,
            'our': 44, 'one': 45, 'other': 46, 'do': 47, 'no': 48, 'information': 49, 'time': 50
        }
        
        # æ‰©å±•æ›´å¤šé«˜é¢‘è¯
        medium_freq_words = {
            'technology': 1500, 'computer': 1800, 'system': 2000, 'business': 2200,
            'development': 2500, 'management': 2800, 'important': 3000, 'different': 3200,
            'following': 3500, 'without': 3800, 'program': 4000, 'problem': 4200,
            'complete': 4500, 'room': 4800, 'until': 5000
        }
        
        low_freq_words = {
            'sophisticated': 8000, 'phenomenon': 8500, 'comprehensive': 7500, 
            'artificial': 6000, 'intelligence': 5500, 'algorithm': 9000,
            'sustainability': 9500, 'unprecedented': 9800, 'anthropic': 9900,
            'outrageous': 8800, 'diagnostic': 8200, 'therapeutic': 9200
        }
        
        return {**common_words, **medium_freq_words, **low_freq_words}
    
    def _load_cefr_levels(self) -> Dict[str, str]:
        """åŠ è½½CEFR (æ¬§æ´²è¯­è¨€å…±åŒå‚è€ƒæ¡†æ¶) è¯æ±‡åˆ†çº§æ•°æ®"""
        return {
            # A1 Level (Beginner)
            'hello': 'A1', 'yes': 'A1', 'no': 'A1', 'please': 'A1', 'thank': 'A1',
            'good': 'A1', 'bad': 'A1', 'big': 'A1', 'small': 'A1', 'new': 'A1',
            
            # A2 Level (Elementary) 
            'important': 'A2', 'different': 'A2', 'possible': 'A2', 'necessary': 'A2',
            'available': 'A2', 'interesting': 'A2', 'difficult': 'A2',
            
            # B1 Level (Intermediate)
            'technology': 'B1', 'computer': 'B1', 'business': 'B1', 'development': 'B1',
            'management': 'B1', 'system': 'B1', 'program': 'B1', 'process': 'B1',
            
            # B2 Level (Upper-Intermediate) 
            'comprehensive': 'B2', 'artificial': 'B2', 'intelligence': 'B2',
            'innovative': 'B2', 'sustainable': 'B2', 'fundamental': 'B2',
            
            # C1 Level (Advanced)
            'sophisticated': 'C1', 'unprecedented': 'C1', 'phenomenon': 'C1',
            'theoretical': 'C1', 'methodology': 'C1', 'implementation': 'C1',
            
            # C2 Level (Proficiency)
            'anthropic': 'C2', 'epistemological': 'C2', 'paradigmatic': 'C2',
            'quintessential': 'C2', 'ubiquitous': 'C2'
        }
    
    def _load_academic_words(self) -> set:
        """åŠ è½½å­¦æœ¯è¯æ±‡åˆ—è¡¨ (Academic Word List)"""
        return {
            'analysis', 'approach', 'area', 'assessment', 'assume', 'authority',
            'available', 'benefit', 'concept', 'consistent', 'constitutional', 'context',
            'contract', 'create', 'data', 'definition', 'derived', 'distribution',
            'economic', 'environment', 'established', 'estimate', 'evidence',
            'export', 'factors', 'financial', 'formula', 'function', 'identified',
            'income', 'indicate', 'individual', 'interpretation', 'involved',
            'issues', 'labor', 'legal', 'legislation', 'major', 'method',
            'occur', 'percent', 'period', 'policy', 'principle', 'procedure',
            'process', 'required', 'research', 'response', 'role', 'section',
            'significant', 'similar', 'source', 'specific', 'structure', 'theory',
            'variables', 'comprehensive', 'sophisticated', 'phenomenon', 'unprecedented'
        }
    
    def assess_word_difficulty(self, word: str, user_level: str) -> Dict[str, any]:
        """
        è¯„ä¼°å•è¯éš¾åº¦
        
        Args:
            word: è¦è¯„ä¼°çš„å•è¯
            user_level: ç”¨æˆ·è‹±è¯­æ°´å¹³ ('4.0', '5.0', '6.0', '7.0')
            
        Returns:
            Dict containing difficulty assessment
        """
        word_lower = word.lower().strip()
        
        # è¿‡æ»¤å¤ªçŸ­çš„è¯æˆ–éå­—æ¯è¯
        if len(word_lower) < 3 or not word_lower.isalpha():
            return {'word': word, 'is_difficult': False, 'reason': 'too_short_or_invalid'}
        
        # ä¼˜å…ˆä½¿ç”¨ä¸“ä¸šæ•°æ®åº“
        if self.use_database and self.vocab_db:
            try:
                db_result = self.vocab_db.get_word_difficulty(word_lower, user_level)
                if db_result:
                    return db_result
            except Exception as e:
                print(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # å¤‡ç”¨åˆ†æé€»è¾‘ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰
        assessment = {
            'word': word,
            'is_difficult': False,
            'difficulty_level': None,
            'cefr_level': None,
            'frequency_rank': None,
            'is_academic': False,
            'definition': None,
            'phonetic': None,
            'reasons': []
        }
        
        # 1. æ£€æŸ¥è¯é¢‘
        if hasattr(self, 'word_frequency'):
            freq_rank = self.word_frequency.get(word_lower)
            if freq_rank:
                assessment['frequency_rank'] = freq_rank
            
        # 2. æ£€æŸ¥CEFRåˆ†çº§
        if hasattr(self, 'cefr_levels'):
            cefr_level = self.cefr_levels.get(word_lower)
            if cefr_level:
                assessment['cefr_level'] = cefr_level
            
        # 3. æ£€æŸ¥æ˜¯å¦ä¸ºå­¦æœ¯è¯æ±‡
        if hasattr(self, 'academic_words') and word_lower in self.academic_words:
            assessment['is_academic'] = True
            assessment['reasons'].append('academic_word')
            
        # 4. åŸºäºç”¨æˆ·æ°´å¹³åˆ¤æ–­æ˜¯å¦å›°éš¾
        user_level_mapping = {
            '4.0': {'max_freq': 1000, 'max_cefr': 'A2'},
            '5.0': {'max_freq': 2000, 'max_cefr': 'B1'},
            '6.0': {'max_freq': 4000, 'max_cefr': 'B2'},
            '7.0': {'max_freq': 6000, 'max_cefr': 'C1'}
        }
        
        user_limits = user_level_mapping.get(user_level, user_level_mapping['6.0'])
        
        # è¯é¢‘åˆ¤æ–­
        freq_rank = assessment.get('frequency_rank')
        cefr_level = assessment.get('cefr_level')
        
        if freq_rank and freq_rank > user_limits['max_freq']:
            assessment['is_difficult'] = True
            assessment['reasons'].append(f'low_frequency_{freq_rank}')
            
        # CEFRçº§åˆ«åˆ¤æ–­
        if cefr_level:
            cefr_order = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
            if cefr_level in cefr_order and user_limits['max_cefr'] in cefr_order:
                if cefr_order.index(cefr_level) > cefr_order.index(user_limits['max_cefr']):
                    assessment['is_difficult'] = True
                    assessment['reasons'].append(f'high_cefr_{cefr_level}')
                    
        # 5. åŸºäºé•¿åº¦çš„å¤‡ç”¨åˆ¤æ–­
        if not assessment['is_difficult'] and len(word_lower) > 8:
            # å¯¹äºæ²¡æœ‰åœ¨æ•°æ®åº“ä¸­çš„é•¿å•è¯ï¼Œè¿›è¡ŒåŸºæœ¬åˆ¤æ–­
            if not freq_rank and not cefr_level:
                assessment['is_difficult'] = True
                assessment['reasons'].append('unknown_long_word')
                
        # 6. è®¾ç½®éš¾åº¦çº§åˆ«
        if assessment['is_difficult']:
            if cefr_level in ['C2']:
                assessment['difficulty_level'] = 'high'
            elif cefr_level in ['C1'] or (freq_rank and freq_rank > 8000):
                assessment['difficulty_level'] = 'advanced'
            else:
                assessment['difficulty_level'] = 'medium'
        
        # 7. å°è¯•è·å–å®šä¹‰å’ŒéŸ³æ ‡
        try:
            synsets = wordnet.synsets(word_lower)
            if synsets:
                assessment['definition'] = synsets[0].definition()
                # ç®€åŒ–çš„éŸ³æ ‡ç”Ÿæˆ
                assessment['phonetic'] = f"/Ëˆ{word_lower}/"
        except:
            pass
            
        return assessment
    
    def analyze_vocabulary_batch(self, words: List[str], user_level: str) -> List[Dict]:
        """æ‰¹é‡åˆ†æè¯æ±‡"""
        results = []
        for word in words:
            assessment = self.assess_word_difficulty(word, user_level)
            if assessment['is_difficult']:
                results.append(assessment)
        return results
    
    def analyze_sentence_complexity(self, sentence: str, user_level: str) -> Dict:
        """åˆ†æå¥å­å¤æ‚åº¦"""
        sentence = sentence.strip()
        
        complexity_assessment = {
            'sentence': sentence,
            'is_complex': False,
            'complexity_level': 'Simple',
            'reasons': [],
            'word_count': 0,
            'difficult_words': []
        }
        
        # è¯æ±‡åŒ– (ä½¿ç”¨ç®€å•åˆ†å‰²)
        words = re.findall(r'\b[a-zA-Z]+\b', sentence.lower())
        words = [word for word in words if word.isalpha()]
        complexity_assessment['word_count'] = len(words)
        
        # æ£€æŸ¥å›°éš¾è¯æ±‡
        difficult_words = []
        for word in set(words):  # å»é‡
            assessment = self.assess_word_difficulty(word, user_level)
            if assessment['is_difficult']:
                difficult_words.append(word)
                
        complexity_assessment['difficult_words'] = difficult_words
        
        # å¤æ‚åº¦æŒ‡æ ‡
        high_complexity_markers = [
            'therefore', 'however', 'nevertheless', 'consequently', 
            'furthermore', 'moreover', 'whereas', 'notwithstanding'
        ]
        
        medium_complexity_markers = [
            'although', 'because', 'since', 'unless', 'whether', 
            'while', 'despite', 'regarding', 'concerning'
        ]
        
        # æ£€æŸ¥å¤æ‚åº¦æ ‡è®°
        for marker in high_complexity_markers:
            if marker in words:
                complexity_assessment['is_complex'] = True
                complexity_assessment['complexity_level'] = 'Highly Complex'
                complexity_assessment['reasons'].append(f'high_complexity_marker_{marker}')
                break
                
        if not complexity_assessment['is_complex']:
            for marker in medium_complexity_markers:
                if marker in words:
                    complexity_assessment['is_complex'] = True
                    complexity_assessment['complexity_level'] = 'Complex'
                    complexity_assessment['reasons'].append(f'medium_complexity_marker_{marker}')
                    break
        
        # åŸºäºé•¿åº¦å’Œæ ‡ç‚¹çš„å¤æ‚åº¦åˆ¤æ–­
        comma_count = sentence.count(',')
        word_count = len(words)
        
        if word_count > 25 and comma_count > 2:
            complexity_assessment['is_complex'] = True
            complexity_assessment['complexity_level'] = 'Highly Complex'
            complexity_assessment['reasons'].append('long_multi_clause')
        elif word_count > 20 and comma_count > 1:
            if not complexity_assessment['is_complex']:
                complexity_assessment['is_complex'] = True
                complexity_assessment['complexity_level'] = 'Complex'
                complexity_assessment['reasons'].append('moderately_long_with_clauses')
        
        # å›°éš¾è¯æ±‡å¯†åº¦
        if len(difficult_words) > 3:
            complexity_assessment['is_complex'] = True
            complexity_assessment['reasons'].append('high_difficult_word_density')
            
        return complexity_assessment

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
vocab_analyzer = VocabularyAnalyzer()

@vocab_bp.route('/api/analyze-vocabulary', methods=['POST'])
def analyze_vocabulary():
    """è¯æ±‡åˆ†æAPIç«¯ç‚¹"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No JSON data provided'
            }), 400
            
        text = data.get('text', '')
        user_level = data.get('user_level', '6.0')
        
        print(f"ğŸ“ æ¥æ”¶åˆ°è¯æ±‡åˆ†æè¯·æ±‚: texté•¿åº¦={len(text)}, user_level={user_level}")
        
        if not text:
            return jsonify({
                'success': False,
                'error': 'No text provided'
            }), 400
        
        # æå–å•è¯ (ä½¿ç”¨ç®€å•åˆ†å‰²é¿å…NLTKé—®é¢˜)
        import re
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        words = [word for word in words if word.isalpha() and len(word) > 3]
        unique_words = list(set(words))
        
        print(f"ğŸ” æå–åˆ° {len(unique_words)} ä¸ªç‹¬ç‰¹å•è¯")
        
        # åˆ†æè¯æ±‡
        difficult_words = vocab_analyzer.analyze_vocabulary_batch(unique_words, user_level)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(difficult_words)} ä¸ªå›°éš¾è¯æ±‡")
        
        # åˆ†æå¥å­
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        
        complex_sentences = []
        for sentence in sentences:
            assessment = vocab_analyzer.analyze_sentence_complexity(sentence, user_level)
            if assessment['is_complex']:
                complex_sentences.append(assessment)
        
        print(f"ğŸ“ æ‰¾åˆ° {len(complex_sentences)} ä¸ªå¤æ‚å¥å­")
        
        result = {
            'success': True,
            'vocabulary': {
                'total_words': len(unique_words),
                'difficult_words': difficult_words[:8],  # é™åˆ¶è¿”å›æ•°é‡
                'difficulty_count': len(difficult_words)
            },
            'sentences': {
                'total_sentences': len(sentences),
                'complex_sentences': complex_sentences[:5],  # é™åˆ¶è¿”å›æ•°é‡
                'complexity_count': len(complex_sentences)
            },
            'user_level': user_level
        }
        
        print("âœ… è¯æ±‡åˆ†æå®Œæˆï¼Œè¿”å›ç»“æœ")
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ è¯æ±‡åˆ†æAPIé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@vocab_bp.route('/api/word-difficulty', methods=['POST'])
def check_word_difficulty():
    """å•è¯éš¾åº¦æ£€æŸ¥API"""
    try:
        data = request.get_json()
        word = data.get('word', '')
        user_level = data.get('user_level', '6.0')
        
        if not word:
            return jsonify({
                'success': False,
                'error': 'No word provided'
            }), 400
        
        assessment = vocab_analyzer.assess_word_difficulty(word, user_level)
        
        return jsonify({
            'success': True,
            'assessment': assessment
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500