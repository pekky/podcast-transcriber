#!/usr/bin/env python3
"""
Vocabulary Analysis Tool for English Learners
ä»éŸ³é¢‘è½¬å½•æ–‡ä»¶ä¸­æå–å’Œåˆ†æé€‚åˆç‰¹å®šè‹±è¯­æ°´å¹³çš„è¯æ±‡

This tool analyzes transcription files and generates vocabulary study guides
with detailed explanations, pronunciations, and examples.
"""

import os
import sys
import re
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
import argparse
from collections import Counter
import nltk
from nltk.corpus import wordnet, cmudict
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag
from nltk.corpus import stopwords
from nltk.parse.stanford import StanfordDependencyParser
import requests
import time
import statistics
from cefr_vocabulary import CEFRVocabulary
from bs4 import BeautifulSoup
import urllib.parse

# Download required NLTK data
def ensure_nltk_data():
    required_data = [
        'punkt', 'punkt_tab', 'averaged_perceptron_tagger', 
        'averaged_perceptron_tagger_eng', 'wordnet', 'cmudict', 'stopwords'
    ]
    
    print("ğŸ“¥ Checking and downloading required NLTK data...")
    for name in required_data:
        try:
            nltk.download(name, quiet=True)
        except Exception as e:
            print(f"   Warning: Could not download {name}: {e}")

ensure_nltk_data()


class VocabularyAnalyzer:
    def __init__(self, target_level: str = "B1+"):
        """
        åˆå§‹åŒ–è¯æ±‡åˆ†æå™¨
        
        Args:
            target_level: ç›®æ ‡è‹±è¯­æ°´å¹³ (CEFRæ ‡å‡†: A1, A2, B1, B1+, B2, C1, C2)
        """
        self.target_level = target_level
        self.pronunciation_dict = cmudict.dict()
        self.stop_words = set(stopwords.words('english'))
        
        # åˆå§‹åŒ–CEFRè¯æ±‡æ•°æ®åº“
        self.cefr_vocab = CEFRVocabulary()
        
        # æ ¹æ®ç›®æ ‡æ°´å¹³ç¡®å®šåŸºç¡€è¯æ±‡ï¼ˆéœ€è¦è¿‡æ»¤çš„ï¼‰
        if target_level in ['A1', 'A2']:
            self.basic_words = set()  # å¯¹äºåˆå­¦è€…ï¼Œä¸è¿‡æ»¤è¯æ±‡
        elif target_level in ['B1', 'B1+']:
            self.basic_words = self.cefr_vocab.get_words_by_level('A1')
        elif target_level == 'B2':
            self.basic_words = self.cefr_vocab.get_words_by_level('A1').union(
                self.cefr_vocab.get_words_by_level('A2')
            )
        else:  # C1, C2
            self.basic_words = self.cefr_vocab.get_all_basic_words().union(
                self.cefr_vocab.get_words_by_level('B1')
            )
        
        # å­˜å‚¨åŸæ–‡æ–‡æœ¬ç”¨äºä¾‹å¥æå–
        self.source_text = ""
        
        # è¯æ±‡éš¾åº¦çº§åˆ«å®šä¹‰
        self.difficulty_levels = {
            'basic': {'min_length': 3, 'max_length': 6, 'common_prefixes': [], 'common_suffixes': []},
            'intermediate': {'min_length': 5, 'max_length': 10, 'common_prefixes': ['un-', 're-', 'pre-'], 'common_suffixes': ['-tion', '-ly', '-ing']},
            'advanced': {'min_length': 7, 'max_length': 15, 'common_prefixes': ['inter-', 'trans-', 'auto-'], 'common_suffixes': ['-ment', '-ism', '-ary']},
            'expert': {'min_length': 8, 'max_length': 20, 'common_prefixes': ['anti-', 'counter-', 'pseudo-'], 'common_suffixes': ['-ology', '-cracy', '-phobia']}
        }
        
        # è¯æ±‡åº“ - æŒ‰éš¾åº¦åˆ†çº§çš„é«˜é¢‘è¯æ±‡
        self.vocabulary_database = self._load_vocabulary_database()
    
    def _load_vocabulary_database(self) -> Dict:
        """åŠ è½½è¯æ±‡æ•°æ®åº“ï¼ˆåŒ…å«è¯æ ¹ã€è¯ç¼€ã€é‡Šä¹‰ç­‰ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºä»å¤–éƒ¨æ–‡ä»¶åŠ è½½æ›´å®Œæ•´çš„è¯æ±‡æ•°æ®åº“
        return {
            # è¯æ ¹è¯ç¼€æ•°æ®åº“
            'roots': {
                'vers': {'meaning': 'turn', 'examples': ['reverse', 'universe', 'adverse']},
                'struct': {'meaning': 'build', 'examples': ['structure', 'construct', 'destructive']},
                'crat': {'meaning': 'rule', 'examples': ['democrat', 'autocrat', 'bureaucrat']},
                'auto': {'meaning': 'self', 'examples': ['automatic', 'autonomous', 'autobiography']},
                'techno': {'meaning': 'technology', 'examples': ['technology', 'technique', 'technical']},
                'hydro': {'meaning': 'water', 'examples': ['hydrogen', 'hydraulic', 'hydrate']},
            },
            'prefixes': {
                'anti-': 'against, opposite',
                'auto-': 'self',
                'inter-': 'between, among',
                'trans-': 'across, through',
                'counter-': 'against, opposite',
                'over-': 'too much, above',
                'under-': 'too little, below',
                'semi-': 'half, partially'
            },
            'suffixes': {
                '-ism': 'doctrine, belief system',
                '-cracy': 'rule, government',
                '-ology': 'study of',
                '-tion': 'action, process',
                '-ment': 'result, action',
                '-ary': 'relating to',
                '-ive': 'tending to',
                '-ous': 'having the quality of'
            }
        }
    
    def extract_text_from_transcript(self, file_path: Path) -> str:
        """ä»è½¬å½•æ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç§»é™¤è¯´è¯äººæ ‡è¯†å’Œç©ºè¡Œ
            lines = content.split('\n')
            text_lines = []
            
            for line in lines:
                line = line.strip()
                if line:
                    # ç§»é™¤è¯´è¯äººæ ‡è¯†
                    line = re.sub(r'^[A-Z]:\s*', '', line)
                    line = re.sub(r'^Speaker:\s*', '', line)
                    line = re.sub(r'^Unknown:\s*', '', line)
                    
                    # ç§»é™¤è¡Œå·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    line = re.sub(r'^\d+â†’', '', line)
                    
                    if line.strip():
                        text_lines.append(line.strip())
            
            result_text = ' '.join(text_lines)
            print(f"ğŸ” æå–çš„æ–‡æœ¬é¢„è§ˆ: {result_text[:200]}...")
            
            # ä¿å­˜åŸæ–‡ç”¨äºä¾‹å¥æå–
            self.source_text = result_text
            
            return result_text
            
        except Exception as e:
            print(f"âŒ Error reading file {file_path}: {e}")
            return ""
    
    def get_pronunciation(self, word: str) -> str:
        """è·å–å•è¯çš„éŸ³æ ‡å‘éŸ³"""
        word_lower = word.lower()
        if word_lower in self.pronunciation_dict:
            # è½¬æ¢CMUéŸ³æ ‡ä¸ºIPAè¿‘ä¼¼æ ¼å¼
            arpabet = self.pronunciation_dict[word_lower][0]
            ipa = self._arpabet_to_ipa(arpabet)
            return f"/{ipa}/"
        return "/pronunciation not found/"
    
    def _arpabet_to_ipa(self, arpabet: List[str]) -> str:
        """å°†CMUéŸ³æ ‡è½¬æ¢ä¸ºIPAè¿‘ä¼¼æ ¼å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è½¬æ¢ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å®Œæ•´çš„æ˜ å°„è¡¨
        mapping = {
            'AA': 'É‘', 'AE': 'Ã¦', 'AH': 'ÊŒ', 'AO': 'É”', 'AW': 'aÊŠ', 'AY': 'aÉª',
            'B': 'b', 'CH': 'tÊƒ', 'D': 'd', 'DH': 'Ã°', 'EH': 'e', 'ER': 'É™r',
            'EY': 'eÉª', 'F': 'f', 'G': 'É¡', 'HH': 'h', 'IH': 'Éª', 'IY': 'i',
            'JH': 'dÊ’', 'K': 'k', 'L': 'l', 'M': 'm', 'N': 'n', 'NG': 'Å‹',
            'OW': 'oÊŠ', 'OY': 'É”Éª', 'P': 'p', 'R': 'r', 'S': 's', 'SH': 'Êƒ',
            'T': 't', 'TH': 'Î¸', 'UH': 'ÊŠ', 'UW': 'u', 'V': 'v', 'W': 'w',
            'Y': 'j', 'Z': 'z', 'ZH': 'Ê’'
        }
        
        result = []
        for phone in arpabet:
            # ç§»é™¤é‡éŸ³æ ‡è®°
            clean_phone = re.sub(r'\d', '', phone)
            if clean_phone in mapping:
                result.append(mapping[clean_phone])
            else:
                result.append(clean_phone.lower())
        
        return ''.join(result)
    
    def analyze_word_difficulty(self, word: str) -> str:
        """åŸºäºCEFRæ ‡å‡†åˆ†æå•è¯éš¾åº¦çº§åˆ«"""
        word_lower = word.lower()
        
        # ä½¿ç”¨CEFRæ•°æ®åº“ç¡®å®šè¯æ±‡çº§åˆ«
        cefr_level = self.cefr_vocab.get_level_for_word(word_lower)
        
        # å°†CEFRçº§åˆ«æ˜ å°„åˆ°ç®€åŒ–éš¾åº¦çº§åˆ«
        cefr_to_difficulty = {
            'A1': 'basic',
            'A2': 'basic', 
            'B1': 'intermediate',
            'B2': 'intermediate',
            'C1': 'advanced',
            'C2': 'expert'
        }
        
        return cefr_to_difficulty.get(cefr_level, 'expert')
    
    def _has_complex_morphology(self, word: str) -> bool:
        """æ£€æŸ¥å•è¯æ˜¯å¦æœ‰å¤æ‚çš„è¯æ±‡å½¢æ€ï¼ˆè¯æ ¹ã€è¯ç¼€ï¼‰"""
        complex_prefixes = ['inter-', 'trans-', 'counter-', 'pseudo-', 'auto-', 'anti-', 'semi-', 'multi-', 'super-', 'ultra-']
        complex_suffixes = ['-tion', '-ism', '-ology', '-cracy', '-ment', '-ness', '-ship', '-ward', '-wise', '-able', '-ible']
        
        word_lower = word.lower()
        
        for prefix in complex_prefixes:
            if word_lower.startswith(prefix.replace('-', '')):
                return True
        
        for suffix in complex_suffixes:
            if word_lower.endswith(suffix.replace('-', '')):
                return True
        
        return False
    
    def _is_common_word_pattern(self, word: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§è¯æ±‡æ¨¡å¼ï¼ˆå¦‚ç®€å•çš„åŠ¨åè¯ã€å½¢å®¹è¯ç­‰ï¼‰"""
        word_lower = word.lower()
        
        # å¸¸è§çš„ç®€å•æ¨¡å¼
        simple_patterns = [
            r'.*ing$',  # ç®€å•è¿›è¡Œæ—¶
            r'.*ed$',   # ç®€å•è¿‡å»å¼
            r'.*er$',   # ç®€å•æ¯”è¾ƒçº§
            r'.*ly$',   # ç®€å•å‰¯è¯
            r'.*s$',    # å¤æ•°å½¢å¼
        ]
        
        for pattern in simple_patterns:
            if re.match(pattern, word_lower):
                return True
        
        return False
    
    def _is_academic_or_technical_word(self, word: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå­¦æœ¯æˆ–æŠ€æœ¯è¯æ±‡"""
        word_lower = word.lower()
        
        # å­¦æœ¯/æŠ€æœ¯è¯æ±‡çš„ç‰¹å¾
        academic_patterns = [
            r'.*ology$',    # å­¦ç§‘åç§°
            r'.*graphy$',   # å­¦ç§‘/æŠ€æœ¯
            r'.*ography$',  # å­¦ç§‘
            r'.*ical$',     # å­¦æœ¯å½¢å®¹è¯
            r'.*istic$',    # å­¦æœ¯å½¢å®¹è¯
            r'.*ization$',  # è¿‡ç¨‹åè¯
            r'.*isation$',  # è¿‡ç¨‹åè¯(è‹±å¼)
        ]
        
        # å­¦æœ¯/æŠ€æœ¯å‰ç¼€
        academic_prefixes = ['bio', 'geo', 'eco', 'techno', 'socio', 'psycho', 'neuro', 'micro', 'macro']
        
        for pattern in academic_patterns:
            if re.match(pattern, word_lower):
                return True
        
        for prefix in academic_prefixes:
            if word_lower.startswith(prefix) and len(word) > len(prefix) + 2:
                return True
        
        return False
    
    def extract_sentences_from_source(self, word: str) -> List[str]:
        """ä»åŸæ–‡ä¸­æå–åŒ…å«æŒ‡å®šè¯æ±‡çš„å¥å­"""
        if not self.source_text:
            return []
        
        sentences = sent_tokenize(self.source_text)
        word_sentences = []
        word_lower = word.lower()
        
        for sentence in sentences:
            # æ£€æŸ¥å¥å­æ˜¯å¦åŒ…å«ç›®æ ‡è¯æ±‡
            sentence_words = word_tokenize(sentence.lower())
            
            # ç²¾ç¡®åŒ¹é…è¯æ±‡ï¼ˆåŒ…æ‹¬è¯æ±‡çš„å˜å½¢ï¼‰
            if any(word_lower in w or w in word_lower for w in sentence_words if len(w) > 3):
                cleaned_sentence = self._clean_sentence_for_example(sentence)
                
                # è¿‡æ»¤æ‰å¤ªé•¿æˆ–å¤ªçŸ­çš„å¥å­
                if 5 <= len(cleaned_sentence.split()) <= 30:
                    word_sentences.append(cleaned_sentence)
        
        # è¿”å›æœ€å¥½çš„ä¾‹å¥ï¼ˆæŒ‰é•¿åº¦å’Œå¤æ‚åº¦æ’åºï¼‰
        word_sentences.sort(key=lambda x: (len(x.split()), x.count(',')))
        return word_sentences[:3]
    
    def _clean_sentence_for_example(self, sentence: str) -> str:
        """æ¸…ç†å¥å­ç”¨ä½œä¾‹å¥"""
        # ç§»é™¤è¯´è¯äººæ ‡è¯†
        sentence = re.sub(r'^[A-Z]:\s*', '', sentence)
        sentence = re.sub(r'^Speaker:\s*', '', sentence)
        sentence = re.sub(r'^Unknown:\s*', '', sentence)
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹
        sentence = re.sub(r'\s+', ' ', sentence)
        sentence = sentence.strip()
        
        # ç¡®ä¿å¥å­ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼Œé€‚å½“çš„æ ‡ç‚¹ç»“å°¾
        if sentence:
            sentence = sentence[0].upper() + sentence[1:]
            if not sentence.endswith(('.', '!', '?')):
                sentence += '.'
        
        return sentence
    
    def fetch_cambridge_dictionary_info(self, word: str) -> Dict:
        """ä»Cambridge Dictionaryè·å–ä¾‹å¥ã€éŸ³æ ‡å’Œè¯æ±‡ä¿¡æ¯"""
        try:
            # URLç¼–ç è¯æ±‡
            encoded_word = urllib.parse.quote(word.lower())
            url = f"https://dictionary.cambridge.org/dictionary/english/{encoded_word}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # åˆå§‹åŒ–è¿”å›ç»“æœ
            result = {
                'examples': [],
                'pronunciation': '',
                'etymology': '',
                'part_of_speech': ''
            }
            
            # è·å–ä¾‹å¥
            example_selectors = [
                '.eg',  # ä¸»è¦ä¾‹å¥é€‰æ‹©å™¨
                '.examp',  # å¤‡ç”¨é€‰æ‹©å™¨
                '.dexamp',  # å¦ä¸€ç§æ ¼å¼
                '.examp .eg'  # åµŒå¥—æ ¼å¼
            ]
            
            for selector in example_selectors:
                example_elements = soup.select(selector)
                for elem in example_elements:
                    example_text = elem.get_text().strip()
                    if example_text and len(example_text.split()) >= 5:
                        # æ¸…ç†ä¾‹å¥
                        example_text = re.sub(r'\s+', ' ', example_text)
                        result['examples'].append(example_text)
                
                if result['examples']:
                    break
            
            # è·å–éŸ³æ ‡ - Cambridgeä½¿ç”¨å¤šç§éŸ³æ ‡é€‰æ‹©å™¨
            pronunciation_selectors = [
                '.ipa',  # IPAéŸ³æ ‡
                '.pron',  # å‘éŸ³
                '.us .pron',  # ç¾å¼å‘éŸ³
                '.uk .pron',  # è‹±å¼å‘éŸ³
                'span[title="International Phonetic Alphabet"]'  # IPAæ ‡é¢˜
            ]
            
            for selector in pronunciation_selectors:
                pron_elem = soup.select_one(selector)
                if pron_elem:
                    pronunciation = pron_elem.get_text().strip()
                    if pronunciation:
                        result['pronunciation'] = f"/{pronunciation}/"
                        break
            
            # è·å–è¯æ€§
            pos_selectors = [
                '.pos',  # è¯æ€§
                '.part-of-speech',  # è¯æ€§
                '.posgram',  # è¯æ€§è¯­æ³•
            ]
            
            for selector in pos_selectors:
                pos_elem = soup.select_one(selector)
                if pos_elem:
                    result['part_of_speech'] = pos_elem.get_text().strip()
                    break
            
            # é™åˆ¶ä¾‹å¥æ•°é‡
            result['examples'] = result['examples'][:3]
            
            return result
            
        except Exception as e:
            print(f"   Warning: Could not fetch dictionary info from Cambridge for '{word}': {e}")
            return {'examples': [], 'pronunciation': '', 'etymology': '', 'part_of_speech': ''}
    
    def fetch_etymology_from_etymonline(self, word: str) -> Dict:
        """ä»Etymology Onlineè·å–è¯¦ç»†è¯æºä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¼”å˜å†å²å’Œç›¸å…³è¯æ±‡"""
        try:
            # URLç¼–ç è¯æ±‡
            encoded_word = urllib.parse.quote(word.lower())
            url = f"https://www.etymonline.com/word/{encoded_word}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            result = {
                'etymology_history': '',
                'root_meaning': '',
                'related_words': [],
                'language_origin': '',
                'evolution_path': []
            }
            
            # æŸ¥æ‰¾ä¸»è¦è¯æºä¿¡æ¯ - Etymology Onlineä½¿ç”¨JSONæ•°æ®
            full_etymology = ""
            
            # ä»é¡µé¢æ–‡æœ¬ä¸­æå–è¯æºä¿¡æ¯
            page_text = soup.get_text()
            
            # æŸ¥æ‰¾å…¸å‹çš„è¯æºæ¨¡å¼
            etymology_patterns = [
                r'late \d+c\..*?from.*?from.*?[.!]',  # late 14c., ... from ... from ...
                r'mid-\d+c\..*?from.*?from.*?[.!]',   # mid-15c., ... from ... from ...
                r'early \d+c\..*?from.*?from.*?[.!]', # early 16c., ... from ... from ...
                r'from.*?from.*?from.*?[.!]',         # from ... from ... from ...
                r'\d+s,.*?from.*?[.!]',               # 1540s, ... from ...
            ]
            
            for pattern in etymology_patterns:
                match = re.search(pattern, page_text, re.DOTALL | re.IGNORECASE)
                if match:
                    full_etymology = match.group(0)
                    # æ¸…ç†å’ŒæˆªçŸ­
                    full_etymology = re.sub(r'\s+', ' ', full_etymology)
                    if len(full_etymology) > 200:
                        full_etymology = full_etymology[:200] + "..."
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¨¡å¼ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾å…³é”®çŸ­è¯­
            if not full_etymology:
                text_parts = page_text.split()
                for i, part in enumerate(text_parts):
                    if 'etymology' in part.lower() or 'late' in part or 'from' in part:
                        # æå–å‘¨å›´çš„æ–‡æœ¬
                        start = max(0, i-5)
                        end = min(len(text_parts), i+20)
                        full_etymology = ' '.join(text_parts[start:end])
                        if len(full_etymology) > 50:
                            break
            
            if full_etymology:
                # æ¸…ç†æ–‡æœ¬
                full_etymology = re.sub(r'\s+', ' ', full_etymology)
                result['etymology_history'] = full_etymology[:300] + "..." if len(full_etymology) > 300 else full_etymology
                
                # æå–è¯­è¨€èµ·æº
                language_patterns = [
                    r'from\s+(Old|Middle|Late|Proto-)?(\w+)\s+',
                    r'(Latin|Greek|French|German|Old English|Proto-Indo-European)\s+',
                    r'via\s+(\w+)\s+'
                ]
                for pattern in language_patterns:
                    match = re.search(pattern, full_etymology, re.IGNORECASE)
                    if match:
                        result['language_origin'] = match.group(0).strip()
                        break
                
                # æå–è¯æ ¹å«ä¹‰
                meaning_patterns = [
                    r'"([^"]+)"',  # å¼•å·å†…çš„å«ä¹‰
                    r'meaning\s+"([^"]+)"',
                    r'literally\s+"([^"]+)"'
                ]
                for pattern in meaning_patterns:
                    match = re.search(pattern, full_etymology)
                    if match:
                        result['root_meaning'] = match.group(1)
                        break
            
            # æŸ¥æ‰¾ç›¸å…³è¯æ±‡
            related_selectors = [
                'a[href*="/word/"]',  # é“¾æ¥åˆ°å…¶ä»–è¯æ±‡çš„æ ‡ç­¾
                '.related-words a',
                '.word-family a'
            ]
            
            related_words = set()
            for selector in related_selectors:
                links = soup.select(selector)
                for link in links[:15]:  # é™åˆ¶æ•°é‡
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    if '/word/' in href and text and text.lower() != word.lower() and len(text) > 2:
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸ç›¸å…³çš„è¯æ±‡
                        if any(char.isalpha() for char in text) and not text.isupper():
                            related_words.add(text.lower())
                        if len(related_words) >= 8:  # é™åˆ¶ç›¸å…³è¯æ±‡æ•°é‡
                            break
            
            result['related_words'] = list(related_words)[:8]
            
            # å°è¯•æå–æ¼”å˜è·¯å¾„
            if full_etymology:
                evolution_matches = re.findall(r'(\d{2,4})\s*[cs]?\s*\.?\s*,?\s*([^,;]+)', full_etymology)
                result['evolution_path'] = [{'period': match[0], 'form': match[1].strip()} 
                                          for match in evolution_matches[:5]]
            
            return result
            
        except Exception as e:
            print(f"   Warning: Could not fetch etymology from Etymonline for '{word}': {e}")
            return {
                'etymology_history': '',
                'root_meaning': '',
                'related_words': [],
                'language_origin': '',
                'evolution_path': []
            }
    
    def fetch_cambridge_examples(self, word: str) -> List[str]:
        """ä»Cambridge Dictionaryè·å–ä¾‹å¥ - ä¿æŒå‘åå…¼å®¹"""
        info = self.fetch_cambridge_dictionary_info(word)
        return info['examples']
    
    def extract_vocabulary(self, text: str) -> List[Tuple[str, str, int]]:
        """ä»æ–‡æœ¬ä¸­æå–è¯æ±‡ï¼Œè¿”å›(å•è¯, CEFRçº§åˆ«, é¢‘æ¬¡)çš„åˆ—è¡¨"""
        # åˆ†è¯å’Œè¯æ€§æ ‡æ³¨
        tokens = word_tokenize(text.lower())
        tagged = pos_tag(tokens)
        
        # è¿‡æ»¤è¯æ±‡
        vocabulary = []
        word_freq = Counter()
        
        # ç¡®å®šç›®æ ‡å­¦ä¹ çº§åˆ«èŒƒå›´
        target_levels = self._get_target_learning_levels()
        
        for word, pos in tagged:
            # åŸºç¡€è¿‡æ»¤æ¡ä»¶
            if (len(word) >= 4 and  # æœ€å°é•¿åº¦è¦æ±‚
                word not in self.stop_words and 
                word not in self.basic_words and  # è¿‡æ»¤å·²æŒæ¡çš„åŸºç¡€è¯æ±‡
                word.isalpha() and
                not word.isupper() and  # é¿å…å…¨å¤§å†™ç¼©ç•¥è¯
                not self._is_proper_noun(word) and  # è¿‡æ»¤ä¸“æœ‰åè¯
                pos in ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']):
                
                # è·å–CEFRçº§åˆ«
                cefr_level = self.cefr_vocab.get_level_for_word(word)
                
                # åªä¿ç•™ç›®æ ‡å­¦ä¹ çº§åˆ«çš„è¯æ±‡
                if cefr_level in target_levels:
                    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯è¿‡äºç®€å•çš„è¯æ±‡å˜å½¢
                    if not self._is_too_simple_variation(word):
                        word_freq[word] += 1
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰é¢‘æ¬¡å’ŒCEFRçº§åˆ«æ’åº
        vocabulary_items = []
        level_priority = {'B2': 3, 'C1': 2, 'C2': 1, 'B1': 4}  # ä¼˜å…ˆçº§æƒé‡
        
        for word, freq in word_freq.most_common():
            cefr_level = self.cefr_vocab.get_level_for_word(word)
            # è®¡ç®—é‡è¦æ€§å¾—åˆ†ï¼ˆé¢‘æ¬¡ + çº§åˆ«æƒé‡ï¼‰
            priority_weight = level_priority.get(cefr_level, 0)
            importance_score = freq + priority_weight
            vocabulary_items.append((word, cefr_level, freq, importance_score))
        
        # æŒ‰é‡è¦æ€§å¾—åˆ†æ’åº
        vocabulary_items.sort(key=lambda x: x[3], reverse=True)
        
        # è¿”å›æ ¼å¼ï¼š(å•è¯, CEFRçº§åˆ«, é¢‘æ¬¡)
        return [(word, cefr_level, freq) for word, cefr_level, freq, _ in vocabulary_items]
    
    def _get_target_learning_levels(self) -> List[str]:
        """æ ¹æ®å½“å‰æ°´å¹³ç¡®å®šç›®æ ‡å­¦ä¹ çº§åˆ«"""
        level_progression = {
            'A1': ['A2', 'B1'],
            'A2': ['B1', 'B2'],
            'B1': ['B2', 'C1'],
            'B1+': ['B2', 'C1'],  # B1+å­¦ä¹ è€…åº”è¯¥å­¦ä¹ B2å’ŒC1è¯æ±‡
            'B2': ['C1', 'C2'],
            'C1': ['C2'],
            'C2': ['C2']  # å·²ç»æœ€é«˜çº§
        }
        
        return level_progression.get(self.target_level, ['B2', 'C1'])
    
    def _is_too_simple_variation(self, word: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè¿‡äºç®€å•çš„è¯æ±‡å˜å½¢"""
        word_lower = word.lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåŸºç¡€è¯çš„ç®€å•å˜å½¢
        if word_lower.endswith('ing'):
            base_word = word_lower[:-3]
            if base_word in self.basic_words:
                return True
        
        if word_lower.endswith('ed'):
            base_word = word_lower[:-2]
            if base_word in self.basic_words:
                return True
        
        if word_lower.endswith('er'):
            base_word = word_lower[:-2]
            if base_word in self.basic_words:
                return True
        
        if word_lower.endswith('ly'):
            base_word = word_lower[:-2]
            if base_word in self.basic_words:
                return True
        
        if word_lower.endswith('s'):
            base_word = word_lower[:-1]
            if base_word in self.basic_words:
                return True
        
        return False
    
    def _is_proper_noun(self, word: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä¸“æœ‰åè¯ï¼ˆåœ°åã€äººåç­‰ï¼‰"""
        # ä¸“æœ‰åè¯é€šå¸¸é¦–å­—æ¯å¤§å†™ï¼Œä¸”å¯èƒ½åŒ…å«å¤šä¸ªå¤§å†™å­—æ¯
        if word[0].isupper():
            # å¸¸è§çš„åœ°åã€å›½å®¶ã€åŸå¸‚ç­‰
            proper_nouns = {
                'shanghai', 'beijing', 'china', 'america', 'europe', 'asia', 'africa', 'russia', 
                'japan', 'india', 'canada', 'australia', 'brazil', 'mexico', 'germany', 
                'france', 'italy', 'spain', 'london', 'paris', 'tokyo', 'moscow', 'delhi',
                'washington', 'california', 'texas', 'florida', 'york', 'trump', 'biden',
                'obama', 'clinton', 'bush', 'reagan', 'kennedy', 'roosevelt', 'lincoln'
            }
            if word.lower() in proper_nouns:
                return True
        return False
    
    def analyze_sentence_difficulty(self, sentence: str) -> Dict:
        """åˆ†æå¥å­çš„å¤æ‚åº¦å’Œéš¾ç‚¹"""
        analysis = {
            'sentence': sentence.strip(),
            'length': len(sentence.split()),
            'difficulty_score': 0,
            'complexity_factors': [],
            'grammar_patterns': [],
            'difficult_words': [],
            'explanation': '',
            'simplified_version': ''
        }
        
        # åŸºç¡€é•¿åº¦åˆ†æ
        word_count = len(sentence.split())
        if word_count > 20:
            analysis['complexity_factors'].append('Long sentence (20+ words)')
            analysis['difficulty_score'] += 2
        elif word_count > 15:
            analysis['complexity_factors'].append('Medium-length sentence (15-20 words)')
            analysis['difficulty_score'] += 1
        
        # è¯æ±‡å¤æ‚åº¦åˆ†æ
        tokens = word_tokenize(sentence.lower())
        for token in tokens:
            if len(token) > 8 and token.isalpha():
                difficulty = self.analyze_word_difficulty(token)
                if difficulty in ['advanced', 'expert']:
                    analysis['difficult_words'].append(token)
                    analysis['difficulty_score'] += 1
        
        # è¯­æ³•ç»“æ„åˆ†æ
        tagged = pos_tag(word_tokenize(sentence))
        analysis['grammar_patterns'] = self._identify_grammar_patterns(tagged)
        
        # å¥å­ç±»å‹åˆ¤æ–­
        sentence_types = self._classify_sentence_type(sentence, tagged)
        analysis['complexity_factors'].extend(sentence_types)
        
        # éš¾åº¦è¯„çº§
        if analysis['difficulty_score'] >= 6:
            analysis['difficulty_level'] = 'Expert'
        elif analysis['difficulty_score'] >= 4:
            analysis['difficulty_level'] = 'Advanced'
        elif analysis['difficulty_score'] >= 2:
            analysis['difficulty_level'] = 'Intermediate'
        else:
            analysis['difficulty_level'] = 'Basic'
        
        # ç”Ÿæˆè§£é‡Šå’Œç®€åŒ–ç‰ˆæœ¬
        analysis['explanation'] = self._explain_sentence_complexity(analysis)
        analysis['simplified_version'] = self._simplify_sentence(sentence)
        
        return analysis
    
    def _identify_grammar_patterns(self, tagged_words: List[Tuple[str, str]]) -> List[str]:
        """è¯†åˆ«è¯­æ³•ç»“æ„æ¨¡å¼"""
        patterns = []
        pos_sequence = [tag for word, tag in tagged_words]
        
        # è¢«åŠ¨è¯­æ€
        if any(tag in ['VBN', 'VBZ'] for tag in pos_sequence):
            for i, tag in enumerate(pos_sequence):
                if tag in ['VBZ', 'VBP', 'VBD'] and i < len(pos_sequence) - 1:
                    if pos_sequence[i + 1] == 'VBN':
                        patterns.append('Passive voice')
                        break
        
        # å¤æ‚æ—¶æ€
        if 'MD' in pos_sequence:  # Modal verbs
            patterns.append('Modal verbs')
        
        # ä»å¥è¯†åˆ«
        subordinating_conjunctions = ['that', 'which', 'who', 'where', 'when', 'while', 'although', 'because', 'since', 'if', 'unless']
        words = [word.lower() for word, tag in tagged_words]
        
        for conjunction in subordinating_conjunctions:
            if conjunction in words:
                patterns.append(f'Subordinate clause ({conjunction})')
                break
        
        # å¤åˆå¥è¯†åˆ«
        coordinating_conjunctions = ['and', 'but', 'or', 'so', 'yet']
        for conjunction in coordinating_conjunctions:
            if conjunction in words:
                patterns.append(f'Compound sentence ({conjunction})')
                break
        
        # å½¢å®¹è¯ä»å¥
        if 'WDT' in pos_sequence or 'WP' in pos_sequence:
            patterns.append('Relative clause')
        
        # åˆ†è¯ç»“æ„
        if 'VBG' in pos_sequence or 'VBN' in pos_sequence:
            patterns.append('Participle construction')
        
        return patterns
    
    def _classify_sentence_type(self, sentence: str, tagged_words: List[Tuple[str, str]]) -> List[str]:
        """åˆ†ç±»å¥å­ç±»å‹"""
        types = []
        
        # ç–‘é—®å¥
        if sentence.strip().endswith('?'):
            types.append('Question')
        
        # æ„Ÿå¹å¥
        if sentence.strip().endswith('!'):
            types.append('Exclamation')
        
        # æ¡ä»¶å¥
        conditional_markers = ['if', 'unless', 'provided', 'supposing', 'assuming']
        words = [word.lower() for word, tag in tagged_words]
        
        for marker in conditional_markers:
            if marker in words:
                types.append('Conditional sentence')
                break
        
        # æ¯”è¾ƒå¥
        comparative_markers = ['than', 'as...as', 'more', 'less', 'better', 'worse']
        for marker in comparative_markers:
            if marker in ' '.join(words):
                types.append('Comparative sentence')
                break
        
        return types
    
    def _explain_sentence_complexity(self, analysis: Dict) -> str:
        """ç”Ÿæˆå¥å­å¤æ‚åº¦è§£é‡Š"""
        explanations = []
        
        if analysis['length'] > 20:
            explanations.append(f"é•¿å¥å­({analysis['length']}ä¸ªå•è¯)éœ€è¦æ³¨æ„å¥å­ç»“æ„")
        
        if analysis['difficult_words']:
            explanations.append(f"åŒ…å«é«˜çº§è¯æ±‡: {', '.join(analysis['difficult_words'][:3])}")
        
        if analysis['grammar_patterns']:
            explanations.append(f"å¤æ‚è¯­æ³•ç»“æ„: {', '.join(analysis['grammar_patterns'][:2])}")
        
        return '; '.join(explanations) if explanations else "ç›¸å¯¹ç®€å•çš„å¥å­ç»“æ„"
    
    def _simplify_sentence(self, sentence: str) -> str:
        """å°è¯•ç®€åŒ–å¥å­ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„NLPå¤„ç†
        simplified = sentence
        
        # ç§»é™¤ä¸€äº›å¤æ‚çš„ä»å¥æ ‡è®°è¯å¹¶ç®€åŒ–
        complex_phrases = {
            'In spite of the fact that': 'Although',
            'Due to the fact that': 'Because',
            'It is important to note that': '',
            'What is particularly interesting is that': 'Notably,',
            'It should be emphasized that': '',
        }
        
        for complex_phrase, simple_phrase in complex_phrases.items():
            simplified = simplified.replace(complex_phrase, simple_phrase)
        
        return simplified.strip()
    
    def extract_difficult_sentences(self, text: str, min_difficulty: str = 'Intermediate') -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–éš¾å¥"""
        sentences = sent_tokenize(text)
        difficult_sentences = []
        
        difficulty_levels = {'Basic': 1, 'Intermediate': 2, 'Advanced': 3, 'Expert': 4}
        min_score = difficulty_levels.get(min_difficulty, 2)
        
        for sentence in sentences:
            # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªç®€å•çš„å¥å­
            if len(sentence.split()) < 8:
                continue
                
            analysis = self.analyze_sentence_difficulty(sentence)
            sentence_score = difficulty_levels.get(analysis['difficulty_level'], 1)
            
            if sentence_score >= min_score:
                difficult_sentences.append(analysis)
        
        # æŒ‰éš¾åº¦æ’åºï¼Œé€‰å–æœ€æœ‰ä»£è¡¨æ€§çš„å¥å­
        difficult_sentences.sort(key=lambda x: x['difficulty_score'], reverse=True)
        
        return difficult_sentences[:20]  # è¿”å›å‰20ä¸ªæœ€éš¾çš„å¥å­
    
    def generate_word_explanation(self, word: str) -> Dict:
        """ç”Ÿæˆå•è¯çš„è¯¦ç»†è§£é‡Š - ä¼˜å…ˆä½¿ç”¨åœ¨çº¿è¯å…¸ä¿¡æ¯"""
        # é¦–å…ˆå°è¯•ä»Cambridge Dictionaryè·å–ä¿¡æ¯
        cambridge_info = self.fetch_cambridge_dictionary_info(word)
        
        explanation = {
            'word': word.lower(),  # å•è¯æ”¹ä¸ºå°å†™æ˜¾ç¤º
            'pronunciation': cambridge_info['pronunciation'] or self.get_pronunciation(word),
            'difficulty': self.analyze_word_difficulty(word),
            'etymology': self._get_enhanced_etymology(word, cambridge_info),
            'definitions': self._get_definitions(word),
            'related_words': self._find_related_words(word),
            'examples': self._generate_examples(word),
            'part_of_speech': cambridge_info['part_of_speech'] or self._get_part_of_speech(word)
        }
        return explanation
    
    def _get_enhanced_etymology(self, word: str, cambridge_info: Dict) -> Dict:
        """è·å–å¢å¼ºçš„è¯æ ¹è¯ç¼€ä¿¡æ¯"""
        # é¦–å…ˆä½¿ç”¨æœ¬åœ°è¯æ ¹è¯ç¼€åˆ†æ
        etymology = self._analyze_etymology(word)
        
        # ä»Etymology Onlineè·å–è¯¦ç»†è¯æºä¿¡æ¯
        online_etymology = self.fetch_etymology_from_etymonline(word)
        if online_etymology and online_etymology.get('etymology_history'):
            etymology.update({
                'etymology_history': online_etymology['etymology_history'],
                'root_meaning': online_etymology['root_meaning'],
                'related_words': online_etymology['related_words'],
                'language_origin': online_etymology['language_origin'],
                'evolution_path': online_etymology['evolution_path']
            })
        
        # å¦‚æœCambridgeæœ‰è¯æºä¿¡æ¯ï¼Œä¹ŸåŠ å…¥
        if cambridge_info.get('etymology'):
            etymology['cambridge_etymology'] = cambridge_info['etymology']
        
        return etymology
    
    def _analyze_etymology(self, word: str) -> Dict:
        """åˆ†æè¯æ±‡çš„è¯æ ¹è¯ç¼€"""
        etymology = {'roots': [], 'prefixes': [], 'suffixes': []}
        
        word_lower = word.lower()
        
        # æ£€æŸ¥å‰ç¼€
        for prefix, meaning in self.vocabulary_database['prefixes'].items():
            prefix_clean = prefix.replace('-', '')
            if word_lower.startswith(prefix_clean):
                etymology['prefixes'].append({'affix': prefix, 'meaning': meaning})
        
        # æ£€æŸ¥åç¼€
        for suffix, meaning in self.vocabulary_database['suffixes'].items():
            suffix_clean = suffix.replace('-', '')
            if word_lower.endswith(suffix_clean):
                etymology['suffixes'].append({'affix': suffix, 'meaning': meaning})
        
        # æ£€æŸ¥è¯æ ¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for root, info in self.vocabulary_database['roots'].items():
            if root in word_lower:
                etymology['roots'].append({'root': root, 'meaning': info['meaning']})
        
        return etymology
    
    def _get_definitions(self, word: str) -> List[str]:
        """è·å–å•è¯å®šä¹‰"""
        definitions = []
        synsets = wordnet.synsets(word)
        
        if synsets:
            # è·å–å‰3ä¸ªæœ€å¸¸è§çš„å®šä¹‰
            for synset in synsets[:3]:
                definitions.append(synset.definition())
        else:
            definitions.append("Definition not found in WordNet")
        
        return definitions
    
    def _find_related_words(self, word: str) -> Dict:
        """æŸ¥æ‰¾ç›¸å…³è¯æ±‡ï¼ˆåŒæ ¹è¯ã€åŒä¹‰è¯ç­‰ï¼‰"""
        related = {'synonyms': [], 'family': []}
        
        # æŸ¥æ‰¾åŒä¹‰è¯
        synsets = wordnet.synsets(word)
        for synset in synsets[:2]:  # é™åˆ¶æ•°é‡
            for lemma in synset.lemmas()[:3]:
                if lemma.name() != word and lemma.name() not in related['synonyms']:
                    related['synonyms'].append(lemma.name().replace('_', ' '))
        
        # æŸ¥æ‰¾è¯æ±‡å®¶æ—ï¼ˆåŸºäºè¯æ ¹ï¼‰
        etymology = self._analyze_etymology(word)
        for root_info in etymology['roots']:
            root = root_info['root']
            if root in self.vocabulary_database['roots']:
                examples = self.vocabulary_database['roots'][root]['examples']
                for example in examples:
                    if example != word and example not in related['family']:
                        related['family'].append(example)
        
        return related
    
    def _generate_examples(self, word: str) -> List[str]:
        """ç”Ÿæˆé«˜è´¨é‡ä¾‹å¥ - ä¼˜å…ˆä½¿ç”¨åŸæ–‡ï¼Œç„¶åä½¿ç”¨Cambridge Dictionary"""
        examples = []
        
        # æ–¹æ³•1: ä»åŸæ–‡ä¸­æå–ä¾‹å¥
        source_examples = self.extract_sentences_from_source(word)
        examples.extend(source_examples)
        
        # æ–¹æ³•2: å¦‚æœåŸæ–‡ä¾‹å¥ä¸å¤Ÿï¼Œä»Cambridge Dictionaryè·å–
        if len(examples) < 3:
            cambridge_examples = self.fetch_cambridge_examples(word)
            examples.extend(cambridge_examples)
        
        # å¦‚æœä»ç„¶ä¸å¤Ÿï¼Œä½¿ç”¨æ”¹è¿›çš„å¤‡ç”¨ä¾‹å¥
        if len(examples) < 3:
            fallback_examples = self._generate_fallback_examples(word)
            examples.extend(fallback_examples)
        
        # å»é‡å¹¶è¿”å›å‰3ä¸ª
        unique_examples = []
        for example in examples:
            if example not in unique_examples:
                unique_examples.append(example)
        
        return unique_examples[:3]
    
    def _generate_fallback_examples(self, word: str) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›çš„å¤‡ç”¨ä¾‹å¥"""
        synsets = wordnet.synsets(word)
        word_lower = word.lower()
        
        if synsets:
            pos = synsets[0].pos()
            
            if pos == 'v':  # åŠ¨è¯
                return [
                    f"Companies need to {word} their strategies regularly.",
                    f"It's essential to {word} these principles effectively.",
                    f"Successful leaders know how to {word} in difficult situations."
                ]
            elif pos == 'a':  # å½¢å®¹è¯
                return [
                    f"The {word} approach has proven successful in many cases.",
                    f"This {word} method addresses the core issues effectively.",
                    f"Experts recommend taking a more {word} stance on this matter."
                ]
            elif pos == 'n':  # åè¯
                return [
                    f"The concept of {word} is fundamental to this field.",
                    f"Recent research has examined the role of {word} in society.",
                    f"Understanding {word} is crucial for academic success."
                ]
        
        # é»˜è®¤é€šç”¨ä¾‹å¥
        return [
            f"The importance of {word} cannot be understated in this context.",
            f"Researchers continue to study {word} and its implications.",
            f"Modern society must consider the impact of {word} carefully."
        ]
    
    def create_study_guide(self, vocabulary: List[Tuple[str, str, int]], 
                          source_file: str, max_words: int = 40, text: str = "") -> str:
        """åˆ›å»ºåŸºäºCEFRæ ‡å‡†çš„è¯æ±‡å­¦ä¹ æŒ‡å—"""
        
        # æŒ‰CEFRçº§åˆ«å’Œé¢‘æ¬¡åˆ†ç»„
        level_groups = {'B2': [], 'C1': [], 'C2': [], 'B1': []}
        
        for word, cefr_level, freq in vocabulary[:max_words]:
            if cefr_level in level_groups:
                level_groups[cefr_level].append((word, freq))
        
        # ç¡®å®šç›®æ ‡çº§åˆ«èŒƒå›´
        target_levels = self._get_target_learning_levels()
        current_level = self.target_level
        
        # ç”Ÿæˆå­¦ä¹ æŒ‡å—å†…å®¹
        guide_content = f"""ğŸ“š CEFR-Based Vocabulary Study Guide
å½“å‰æ°´å¹³: {current_level} | ç›®æ ‡å­¦ä¹ çº§åˆ«: {' & '.join(target_levels)}

æºæ–‡ä»¶: {source_file}

==============================================================

ğŸ”¤ VOCABULARY LIST ORGANIZED BY CEFR LEVELS

"""
        
        # æŒ‰CEFRçº§åˆ«ç»„ç»‡è¯æ±‡å±•ç¤º
        word_count = 1
        
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºå±•ç¤ºå„çº§åˆ«è¯æ±‡
        priority_levels = ['C1', 'B2', 'C2', 'B1']  # C1ä¼˜å…ˆï¼Œå› ä¸ºæ˜¯ä¸»è¦ç›®æ ‡
        
        for level in priority_levels:
            if level_groups[level]:
                guide_content += f"\nğŸ¯ {level} Level Words ({len(level_groups[level])} words)\n"
                guide_content += "=" * 50 + "\n\n"
                
                for word, freq in level_groups[level]:
                    explanation = self.generate_word_explanation(word)
                    
                    guide_content += f"""{word_count}. {explanation['word']} {explanation['pronunciation']} [{level}]
ã€å‡ºç°é¢‘æ¬¡ã€‘{freq} æ¬¡
ã€è¯æ ¹è¯ç¼€ã€‘{self._format_etymology(explanation['etymology'])}
ã€è¯æ€§ã€‘{explanation['part_of_speech']}
ã€é‡Šä¹‰ã€‘{explanation['definitions'][0] if explanation['definitions'] else 'Definition not available'}
ã€åŒæ ¹è¯ã€‘{', '.join(explanation['related_words']['family'][:3]) if explanation['related_words']['family'] else 'No related words found'}
ã€ä¾‹å¥ã€‘
"""
                    
                    for i, example in enumerate(explanation['examples'], 1):
                        guide_content += f"- {example}\n"
                    
                    guide_content += "\n"
                    word_count += 1
        
        # æ·»åŠ CEFRçº§åˆ«ç»Ÿè®¡
        guide_content += f"""
==============================================================

ğŸ“Š CEFR LEVEL DISTRIBUTION
è¿™äº›è¯æ±‡æŒ‰CEFRçº§åˆ«åˆ†å¸ƒç»Ÿè®¡ï¼š

ğŸŸ¦ C1 ADVANCED ({len(level_groups['C1'])} words) - é«˜çº§è¯æ±‡ï¼Œæå‡å­¦æœ¯å’Œä¸“ä¸šè¡¨è¾¾
{', '.join([word for word, freq in level_groups['C1'][:10]])}

ğŸŸ¨ B2 UPPER-INTERMEDIATE ({len(level_groups['B2'])} words) - ä¸­é«˜çº§è¯æ±‡ï¼Œæ—¥å¸¸å’Œå·¥ä½œå¿…å¤‡
{', '.join([word for word, freq in level_groups['B2'][:10]])}

ğŸŸª C2 PROFICIENCY ({len(level_groups['C2'])} words) - æ¥è¿‘æ¯è¯­è€…æ°´å¹³çš„é«˜éš¾è¯æ±‡
{', '.join([word for word, freq in level_groups['C2'][:5]])}

==============================================================

ğŸ“ DIFFICULT SENTENCES ANALYSIS
ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­çš„å¤æ‚å¥å­ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„è¯­æ³•ç»“æ„å’Œç†è§£éš¾ç‚¹ï¼š

"""
        
        # æ·»åŠ éš¾å¥åˆ†æ
        if text:
            difficult_sentences = self.extract_difficult_sentences(text, 'Intermediate')
            
            for i, sentence_analysis in enumerate(difficult_sentences[:10], 1):
                guide_content += f"""
{i}. ã€{sentence_analysis['difficulty_level']}ã€‘ {sentence_analysis['sentence']}

   ğŸ“Š å¤æ‚åº¦åˆ†æ:
   - å¥å­é•¿åº¦: {sentence_analysis['length']} ä¸ªå•è¯
   - éš¾åº¦è¯„åˆ†: {sentence_analysis['difficulty_score']}/10
   - å¤æ‚å› ç´ : {', '.join(sentence_analysis['complexity_factors']) if sentence_analysis['complexity_factors'] else 'æ— ç‰¹æ®Šå¤æ‚å› ç´ '}
   
   ğŸ” è¯­æ³•ç»“æ„: {', '.join(sentence_analysis['grammar_patterns']) if sentence_analysis['grammar_patterns'] else 'åŸºç¡€è¯­æ³•ç»“æ„'}
   
   ğŸ’¡ ç†è§£è¦ç‚¹: {sentence_analysis['explanation']}
   
   âœï¸  ç®€åŒ–ç†è§£: {sentence_analysis['simplified_version']}

"""
        
        guide_content += f"""
==============================================================

ğŸ’¡ STUDY TIPS

**è¯æ±‡å­¦ä¹ ç­–ç•¥:**
1. **è¯æ ¹è®°å¿†æ³•**: é‡ç‚¹å…³æ³¨è¯æ ¹è¯ç¼€ï¼Œå¦‚ -crat (ç»Ÿæ²»), auto- (è‡ªå·±), -ism (ä¸»ä¹‰)
2. **è¯­å¢ƒå­¦ä¹ **: ç»“åˆéŸ³é¢‘å†…å®¹ç†è§£è¯æ±‡åœ¨è®¨è®ºä¸­çš„ä½¿ç”¨
3. **åŒä¹‰æ›¿æ¢**: å­¦ä¹ é«˜çº§è¯æ±‡æ›¿æ¢åŸºç¡€è¯æ±‡
4. **å†™ä½œåº”ç”¨**: è¿™äº›è¯æ±‡ç‰¹åˆ«é€‚ç”¨äºå­¦æœ¯å†™ä½œå’Œæ­£å¼è®¨è®º

**å¥å­ç†è§£æŠ€å·§:**
1. **ç»“æ„åˆ†æ**: å…ˆæ‰¾ä¸»è°“å®¾ï¼Œå†åˆ†æä»å¥å’Œä¿®é¥°æˆåˆ†
2. **æ–­å¥ç»ƒä¹ **: é•¿å¥å­å¯ä»¥æŒ‰ç…§è¯­æ³•ç»“æ„è¿›è¡Œæ–­å¥ç†è§£
3. **è¯­æ³•æ¨¡å¼**: ç†Ÿæ‚‰å¸¸è§çš„å¤æ‚è¯­æ³•ç»“æ„ï¼ˆè¢«åŠ¨è¯­æ€ã€ä»å¥ç­‰ï¼‰
4. **ç®€åŒ–å¯¹æ¯”**: å¯¹ç…§ç®€åŒ–ç‰ˆæœ¬ç†è§£å¤æ‚å¥å­çš„æ ¸å¿ƒæ„æ€

==============================================================

ğŸ¯ RECOMMENDED NEXT STEPS

**è¯æ±‡æå‡:**
1. å¬åŸéŸ³é¢‘ï¼Œæ³¨æ„è¿™äº›è¯æ±‡çš„å‘éŸ³å’Œè¯­è°ƒ
2. åˆ¶ä½œå•è¯å¡ç‰‡ï¼Œæ­£é¢å†™è¯æ±‡ï¼Œåé¢å†™é‡Šä¹‰å’Œä¾‹å¥
3. å°è¯•ç”¨è¿™äº›è¯æ±‡å†™ä¸€ç¯‡ç›¸å…³ä¸»é¢˜çš„çŸ­æ–‡
4. å®šæœŸå¤ä¹ ï¼Œç‰¹åˆ«å…³æ³¨é«˜ä¼˜å…ˆçº§è¯æ±‡

**å¥å­åˆ†æ:**
1. æ¯å¤©åˆ†æ2-3ä¸ªå¤æ‚å¥å­ï¼Œç†è§£å…¶è¯­æ³•ç»“æ„
2. ç»ƒä¹ é•¿å¥å­çš„æ–­å¥å’Œç†è§£
3. å°è¯•æ”¹å†™å¤æ‚å¥å­ä¸ºç®€å•å¥å­
4. åœ¨å†™ä½œä¸­æ¨¡ä»¿è¿™äº›é«˜çº§å¥å‹ç»“æ„

Generated by CEFR-Based Vocabulary & Sentence Analysis Tool
Current Level: {self.target_level} | Target Learning Levels: {' & '.join(target_levels)}
Total Vocabulary Analyzed: {len(vocabulary)} words
Analysis Date: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        return guide_content
    
    def _format_etymology(self, etymology: Dict) -> str:
        """æ ¼å¼åŒ–è¯æºä¿¡æ¯"""
        parts = []
        
        # åœ¨çº¿è¯æºå†å²ï¼ˆä¼˜å…ˆæ˜¾ç¤ºï¼‰
        if etymology.get('etymology_history'):
            parts.append(f"ğŸ“œ è¯æºå†å²: {etymology['etymology_history']}")
        
        # è¯­è¨€èµ·æº
        if etymology.get('language_origin'):
            parts.append(f"ğŸŒ è¯­è¨€èµ·æº: {etymology['language_origin']}")
        
        # è¯æ ¹å«ä¹‰
        if etymology.get('root_meaning'):
            parts.append(f"ğŸ”¤ è¯æ ¹å«ä¹‰: {etymology['root_meaning']}")
        
        # æ¼”å˜è·¯å¾„
        if etymology.get('evolution_path') and etymology['evolution_path']:
            evolution_str = " â†’ ".join([f"{e['period']}: {e['form']}" for e in etymology['evolution_path']])
            parts.append(f"ğŸ“ˆ æ¼”å˜è·¯å¾„: {evolution_str}")
        
        # ç›¸å…³åŒæ ¹è¯
        if etymology.get('related_words') and etymology['related_words']:
            related_str = ', '.join(etymology['related_words'])
            parts.append(f"ğŸ”— åŒæ ¹è¯æ±‡: {related_str}")
        
        # æœ¬åœ°è¯ç¼€åˆ†æï¼ˆä½œä¸ºè¡¥å……ï¼‰
        local_parts = []
        if etymology.get('prefixes'):
            local_parts.extend([f"{p['affix']} ({p['meaning']})" for p in etymology['prefixes']])
        
        if etymology.get('roots'):
            local_parts.extend([f"{r['root']}- ({r['meaning']})" for r in etymology['roots']])
        
        if etymology.get('suffixes'):
            local_parts.extend([f"{s['affix']} ({s['meaning']})" for s in etymology['suffixes']])
        
        if local_parts:
            parts.append(f"ğŸ§© è¯ç¼€åˆ†æ: {' + '.join(local_parts)}")
        
        return '\n'.join(parts) if parts else 'Etymology not available'
    
    def _get_part_of_speech(self, word: str) -> str:
        """è·å–è¯æ€§"""
        synsets = wordnet.synsets(word)
        if synsets:
            pos_mapping = {
                'n': 'noun',
                'v': 'verb', 
                'a': 'adjective',
                's': 'adjective',
                'r': 'adverb'
            }
            return pos_mapping.get(synsets[0].pos(), 'unknown')
        return 'unknown'
    
    def analyze_transcript_file(self, file_path: Path, output_dir: Optional[Path] = None) -> Path:
        """åˆ†æè½¬å½•æ–‡ä»¶å¹¶ç”Ÿæˆè¯æ±‡å­¦ä¹ æŒ‡å—"""
        print(f"ğŸ“– åˆ†æè½¬å½•æ–‡ä»¶: {file_path.name}")
        
        # æå–æ–‡æœ¬
        text = self.extract_text_from_transcript(file_path)
        if not text:
            raise ValueError("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬")
        
        print(f"âœ… æå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # æå–è¯æ±‡
        print("ğŸ” æå–å’Œåˆ†æè¯æ±‡...")
        vocabulary = self.extract_vocabulary(text)
        
        print(f"âœ… è¯†åˆ«å‡º {len(vocabulary)} ä¸ªé€‚åˆå­¦ä¹ çš„è¯æ±‡")
        
        # ç”Ÿæˆå­¦ä¹ æŒ‡å—
        print("ğŸ“ ç”Ÿæˆè¯æ±‡å’Œå¥å­å­¦ä¹ æŒ‡å—...")
        guide_content = self.create_study_guide(vocabulary, file_path.name, max_words=40, text=text)
        
        # ä¿å­˜æ–‡ä»¶
        if output_dir is None:
            output_dir = file_path.parent
        
        output_file = output_dir / f"{file_path.stem}_Vocabulary_Study.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(guide_content)
        
        print(f"ğŸ’¾ å­¦ä¹ æŒ‡å—å·²ä¿å­˜: {output_file}")
        return output_file


def main():
    parser = argparse.ArgumentParser(
        description='åˆ†æéŸ³é¢‘è½¬å½•æ–‡ä»¶å¹¶ç”Ÿæˆè¯æ±‡å­¦ä¹ æŒ‡å—',
        epilog='æ”¯æŒä»podcastè½¬å½•æ–‡ä»¶ä¸­æå–é€‚åˆç‰¹å®šè‹±è¯­æ°´å¹³çš„è¯æ±‡è¿›è¡Œå­¦ä¹ '
    )
    
    parser.add_argument('input', help='è¾“å…¥çš„è½¬å½•æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½• (é»˜è®¤: ä¸è¾“å…¥æ–‡ä»¶åŒç›®å½•)')
    parser.add_argument('-l', '--level', default='6.0-6.5', 
                       help='ç›®æ ‡è‹±è¯­æ°´å¹³ (é»˜è®¤: 6.0-6.5)')
    parser.add_argument('-m', '--max-words', type=int, default=40,
                       help='æœ€å¤§è¯æ±‡æ•°é‡ (é»˜è®¤: 40)')
    
    args = parser.parse_args()
    
    input_file = Path(args.input)
    
    if not input_file.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        sys.exit(1)
    
    if not input_file.suffix.lower() == '.txt':
        print(f"âŒ åªæ”¯æŒ.txtæ ¼å¼çš„è½¬å½•æ–‡ä»¶")
        sys.exit(1)
    
    output_dir = Path(args.output) if args.output else None
    
    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        print(f"ğŸš€ åˆå§‹åŒ–è¯æ±‡åˆ†æå™¨ (ç›®æ ‡æ°´å¹³: {args.level})")
        analyzer = VocabularyAnalyzer(args.level)
        
        # åˆ†ææ–‡ä»¶
        result_file = analyzer.analyze_transcript_file(
            input_file, 
            output_dir
        )
        
        print(f"\nğŸ‰ è¯æ±‡åˆ†æå®Œæˆ!")
        print(f"ğŸ“ å­¦ä¹ æŒ‡å—æ–‡ä»¶: {result_file}")
        print(f"ğŸ’¡ å»ºè®®ç»“åˆåŸéŸ³é¢‘æ–‡ä»¶è¿›è¡Œå­¦ä¹ ï¼Œæ•ˆæœæ›´ä½³!")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()